{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import unicodedata\n",
    "import re\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                   if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_markup(s):\n",
    "    tag = False\n",
    "    quote = False\n",
    "    out = \"\"\n",
    "    for c in s:\n",
    "        if c == '<' and not quote:\n",
    "            tag = True\n",
    "        elif c == '>' and not quote:\n",
    "            tag = False\n",
    "        elif (c == '\"' or c == \"'\") and tag:\n",
    "            quote = not quote\n",
    "        elif not tag:\n",
    "            out = out + c\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(w):\n",
    "    w = remove_html_markup(w)\n",
    "    w = w.lower().strip()\n",
    "    if w == '':\n",
    "        return 0\n",
    "    else:\n",
    "        w = unicode_to_ascii(w)\n",
    "        w = re.sub(r\"[^a-z]+\", \" \", w)\n",
    "        w = w.strip()\n",
    "        w = re.sub(r'\\s+', ' ', w)\n",
    "    w = w.split(' ')\n",
    "    w = [i for i in w if i != '']\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    w = [i for i in w if i not in stopwords]\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    w = [stemmer.stem(i) for i in w]\n",
    "    w = ' '.join(w)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lines_to_text(lines, sep):\n",
    "    text = ''\n",
    "    for i in range(len(lines)):\n",
    "        if i == len(lines) - 1:\n",
    "            text += str(lines[i])\n",
    "        else:\n",
    "            text += str(lines[i]) + sep\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_frequent_words(unique_words):\n",
    "    new_unique_words = ['unk']\n",
    "    for i in list(unique_words.keys()):\n",
    "        if unique_words[i] > 5:\n",
    "            new_unique_words.append(i)\n",
    "    return new_unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(pos_lines, neg_lines):\n",
    "    lines = pos_lines + neg_lines\n",
    "    text = lines_to_text(lines, ' ')\n",
    "    unique_words = Counter(text.split(' '))\n",
    "    print('No. of unique words in dataset: ', len(unique_words.keys()))\n",
    "    print()\n",
    "    unique_words = retrieve_frequent_words(unique_words)\n",
    "    print('New no. of unique words in dataset: ', len(unique_words))\n",
    "    print()\n",
    "    word_index = {i: unique_words.index(i) for i in unique_words}\n",
    "    return word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(lines):\n",
    "    new_lines = []\n",
    "    for i in lines:\n",
    "        s = preprocess_sentence(i)\n",
    "        if s != 0:\n",
    "            new_lines.append(s)\n",
    "    return new_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_retrieve(file_names):\n",
    "    text_files = []\n",
    "    for i in file_names:\n",
    "        with open(i, 'r') as f:\n",
    "            text_files.append(f.read())\n",
    "        f.close()\n",
    "    return text_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files():\n",
    "    path = '/Users/preethamganesh/Downloads/aclImdb/'\n",
    "    pos_files_train = [os.path.join(path + 'train/pos/', f) for f in os.listdir(path + 'train/pos/') if\n",
    "                       os.path.isfile(os.path.join(path + 'train/pos/', f))]\n",
    "    pos_files_test = [os.path.join(path + 'test/pos/', f) for f in os.listdir(path + 'test/pos/') if\n",
    "                      os.path.isfile(os.path.join(path + 'test/pos/', f))]\n",
    "    pos_files = pos_files_train + pos_files_test\n",
    "    neg_files_train = [os.path.join(path + 'train/neg/', f) for f in os.listdir(path + 'train/neg/') if\n",
    "                       os.path.isfile(os.path.join(path + 'train/neg/', f))]\n",
    "    neg_files_test = [os.path.join(path + 'test/neg/', f) for f in os.listdir(path + 'test/neg/') if\n",
    "                      os.path.isfile(os.path.join(path + 'test/neg/', f))]\n",
    "    neg_files = neg_files_train + neg_files_test\n",
    "    print('No. of positive reviews: ', len(pos_files))\n",
    "    print('No. of negative reviews: ', len(neg_files))\n",
    "    print()\n",
    "    return pos_files, neg_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_probability(word, lines, vocabulary):\n",
    "    if word.lower() in vocabulary:\n",
    "        text = lines_to_text(lines, ' ')\n",
    "        unique_words = Counter(text.split(' '))\n",
    "        return unique_words[word]/len(text.split(' '))\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unk_words(lines, vocabulary):\n",
    "    new_lines = []\n",
    "    for i in lines:\n",
    "        new_line = []\n",
    "        for j in i.split(' '):\n",
    "            if j in vocabulary:\n",
    "                new_line.append(j)\n",
    "            else:\n",
    "                new_line.append('unk')\n",
    "        line = ' '.join(new_line)\n",
    "        new_lines.append(line)\n",
    "    return new_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(train, test):\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(train)\n",
    "    train = vectorizer.transform(train)\n",
    "    test = vectorizer.transform(test)\n",
    "    train = train.toarray()\n",
    "    test = test.toarray()\n",
    "    return train, test, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probability(x, mean, stdev):\n",
    "    exponent = math.exp(-((x - mean) ** 2 / (2 * stdev ** 2)))\n",
    "    return (1 / (math.sqrt(2 * math.pi) * stdev)) * exponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_separate(train_x, train_y):\n",
    "    classes = np.unique(train_y)\n",
    "    class_index = {}\n",
    "    subdata = {}\n",
    "    cls, counts = np.unique(train_y, return_counts=True)\n",
    "    class_frequency = dict(zip(cls, counts))\n",
    "    new_class_frequency = dict(zip(cls, counts))\n",
    "    for i in classes:\n",
    "        class_index[i] = np.argwhere(train_y == i)\n",
    "        subdata[i] = train_x[class_index[i], :]\n",
    "        subdata[i] = np.reshape(subdata[i], [subdata[i].shape[0], subdata[i].shape[2]])\n",
    "        new_class_frequency[i] = class_frequency[i] / sum(list(class_frequency.values()))\n",
    "    return subdata, new_class_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(train_x, train_y):\n",
    "    separated_dataset, class_frequency = class_separate(train_x, train_y)\n",
    "    classes = np.unique(train_y)\n",
    "    means, stdev = {}, {}\n",
    "    for i in classes:\n",
    "        means[i] = np.mean(separated_dataset[i], axis=0)\n",
    "        stdev[i] = np.std(separated_dataset[i], axis=0)\n",
    "    return means, stdev, class_frequency, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(x, means, stdev, class_frequency, classes):\n",
    "    probability = {i: 1 for i in classes}\n",
    "    for i in classes:\n",
    "        for j in range(len(means)):\n",
    "            probability[i] *= calculate_probability(x[j], means[i][j], stdev[i][j])\n",
    "    probability = {i: probability[i] for i in probability}\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_x, means, stdev, class_frequency, classes):\n",
    "    pred = []\n",
    "    for i in test_x:\n",
    "        pred_class, max_prob = None, 0\n",
    "        for class_index, prob in predict_proba(i, means, stdev, class_frequency, classes).items():\n",
    "            if prob > max_prob:\n",
    "                max_prob = prob\n",
    "                pred_class = class_index\n",
    "        pred.append(pred_class)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(true, pred):\n",
    "    acc = 0\n",
    "    for i, j in zip(true, pred):\n",
    "        try:\n",
    "            if int(i) == int(j):\n",
    "                acc += 1\n",
    "        except:\n",
    "            continue\n",
    "    return acc/len(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(data, n_cross_val):\n",
    "    data_split = np.array_split((data), n_cross_val)\n",
    "    return data_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(train, test):\n",
    "    n_cross_val = 5\n",
    "    train_index = cross_validation(train, n_cross_val)\n",
    "    items = []\n",
    "    accuracy = []\n",
    "    for i in range(len(train_index)):\n",
    "        val = train_index[i]\n",
    "        new_train = train.drop(val.index)\n",
    "        train_x, val_x, vectorizer = tokenize(new_train['sentences'], val['sentences'])\n",
    "        train_y = np.array(new_train['sentiment'])\n",
    "        val_y = np.array(val['sentiment'])\n",
    "        means, stdev, class_frequency, classes = model_training(train_x, train_y)\n",
    "        val_pred = predict(val_x, means, stdev, class_frequency, classes)\n",
    "        val_acc = accuracy_score(val_y, val_pred)\n",
    "        accuracy.append(val_acc)\n",
    "        items.append([means, stdev, class_frequency, classes, vectorizer])\n",
    "        print('Validation Accuracy for iteration ' + str(i) + ' = ' + str(round(val_acc, 3)))\n",
    "    min_index = accuracy.index(min(accuracy))\n",
    "    new_items = items[min_index]\n",
    "    test_x = new_items[4].transform(test['sentences'])\n",
    "    test_x = test_x.toarray()\n",
    "    test_pred = predict(test_x, new_items[0], new_items[1], new_items[2], new_items[3])\n",
    "    test_y = np.array(test['sentiment'])\n",
    "    test_acc = accuracy_score(test_y, test_pred)\n",
    "    print()\n",
    "    print('Test accuracy = ', round(test_acc, 3))\n",
    "    print()\n",
    "    words = new_items[4].get_feature_names()\n",
    "    pos_mean = means[1]\n",
    "    neg_mean = means[0]\n",
    "    pos_ind = pos_mean.argsort()\n",
    "    neg_ind = neg_mean.argsort()\n",
    "    pos_words = list(zip(*(sorted(zip(pos_mean, words)))))[1]\n",
    "    neg_words = list(zip(*(sorted(zip(neg_mean, words)))))[1]\n",
    "    print('Top 10 positive words: ', pos_words[:10])\n",
    "    print('Top 10 negative words: ', neg_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of positive reviews:  25000\n",
      "No. of negative reviews:  25000\n",
      "\n",
      "New no. of positive reviews after data preprocessing:  25000\n",
      "New no. of negative reviews after data preprocessing:  25000\n"
     ]
    }
   ],
   "source": [
    "pos_files, neg_files = find_files()\n",
    "pos_text = text_retrieve(pos_files)\n",
    "neg_text = text_retrieve(neg_files)\n",
    "pos_lines = create_dataset(pos_text)\n",
    "print('New no. of positive reviews after data preprocessing: ', len(pos_lines))\n",
    "neg_lines = create_dataset(neg_text)\n",
    "print('New no. of negative reviews after data preprocessing: ', len(neg_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of unique words in dataset:  70842\n",
      "\n",
      "New no. of unique words in dataset:  24437\n",
      "\n",
      "Unknown words in positive sentences\n",
      "Unknown words in negative sentences\n"
     ]
    }
   ],
   "source": [
    "vocabulary = create_vocabulary(pos_lines, neg_lines)\n",
    "pos_lines = remove_unk_words(pos_lines, vocabulary)\n",
    "print('Unknown words in positive sentences')\n",
    "neg_lines = remove_unk_words(neg_lines, vocabulary)\n",
    "print('Unknown words in negative sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of positive reviews in training set:  22500\n",
      "No. of negative reviews in training set:  22500\n",
      "No. of positive reviews in testing set:  2500\n",
      "No. of negative reviews in testing set:  2500\n"
     ]
    }
   ],
   "source": [
    "train_pos, test_pos = train_test_split(pos_lines, test_size=0.1)\n",
    "train_neg, test_neg = train_test_split(neg_lines, test_size=0.1)\n",
    "print('No. of positive reviews in training set: ', len(train_pos))\n",
    "print('No. of negative reviews in training set: ', len(train_neg))\n",
    "print('No. of positive reviews in testing set: ', len(test_pos))\n",
    "print('No. of negative reviews in testing set: ', len(test_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = {}\n",
    "train['sentences'] = train_pos\n",
    "train['sentiment'] = [1 for i in range(len(train_pos))]\n",
    "train['sentences'] = train['sentences'] + train_neg\n",
    "train['sentiment'] = train['sentiment'] + [0 for i in range(len(train_neg))]\n",
    "test = {}\n",
    "test['sentences'] = test_pos\n",
    "test['sentiment'] = [1 for i in range(len(test_pos))]\n",
    "test['sentences'] = test['sentences'] + test_neg\n",
    "test['sentiment'] = test['sentiment'] + [0 for i in range(len(test_neg))]\n",
    "train = pd.DataFrame(train, columns=['sentences', 'sentiment'])\n",
    "train = shuffle(train)\n",
    "test = pd.DataFrame(test, columns=['sentences', 'sentiment'])\n",
    "test = shuffle(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy for iteration 0 = 0.502\n",
      "Validation Accuracy for iteration 1 = 0.504\n",
      "Validation Accuracy for iteration 2 = 0.503\n",
      "Validation Accuracy for iteration 3 = 0.502\n",
      "Validation Accuracy for iteration 4 = 0.493\n",
      "\n",
      "Test accuracy =  0.5\n",
      "\n",
      "Top 10 positive words:  ('acacia', 'acharya', 'acromegali', 'aden', 'adjl', 'adonijah', 'advani', 'advisori', 'agey', 'agren')\n",
      "Top 10 negative words:  ('abhay', 'abolitionist', 'acquiesc', 'acrobatti', 'aday', 'ade', 'adelin', 'adentro', 'adroit', 'adventist')\n"
     ]
    }
   ],
   "source": [
    "naive_bayes(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook can be downloaded at https://github.com/preetham7897/website/blob/master/documents/ganesh_03.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
